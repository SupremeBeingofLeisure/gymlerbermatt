\section{Berechnung des Ranking Eigenvektors}

Wir skizzieren grob die Idee der power method zur Berechnung eines Eigenvektors einer Matrix $\B{M}$: Man startet mit einem \glqq Guess\grqq\ für ein Ranking, $\B{x}_0$. Sodann generiert man die Folge
$$\B{x}_k=\B{Mx}_{k-1}$$
was $\B{x}_k=\B{M}^k\B{x}_0$ entspricht --- daher der Name power method --- und lässt $k$ gegen Unendlich gehen. Der Vektor $\B{x}_k$ ist eine gute Approximation eines Eigenvektors zum dominanten Eigenwert von $\B{M}$.

Leider kann es vorkommen, dass die Folge nicht \glqq schön\grqq\ konvergiert, beispielsweise unbegrenzt wächst oder in $\B{0}$ zusammenfällt. Abhilfe kann dann in wohlwollenden Fällen eine Reskalierung mit einer Norm schaffen. Zum Beispiel indem man nach jedem Schritt mit etwa
$$\B{x}_k=\frac{\B{M}\B{x}_{k-1}}{\norm{\B{M}\B{x}_{k-1}}}$$
neu skaliert.
Diese Methode setzt üblicherweise voraus, dass der korrespondierende Eigenraum die Dimension $1$ hat, was durch die Matrix $\B{M}$ erfüllt ist.

Um die power method auf Matrizen $\B{M}$ unseres Ranking-Problems anzuwenden, wäre es bequem, wenn alle andern Eigenwerte von $\B{M}$ die Bedingung $\abs{\gl}<1$ erfüllten. Unter dieser Bedingung konvergiert nämlich die Methode gegen den gewünschten Eigenvektor, wie wir unten zeigen. Dazu brauchen wir

\begin{defn}
Unter der 1-Norm eines Vektors $\B{v}$ verstehen wir $\norm{\B{v}}_1=\sum_i\abs{v_i}$.
\end{defn}

\begin{prop}\label{satzbound}
Sei $\B{M}$ eine positive spalten-stochastische $n\times n$ Matrix und $V$ der Unterraum von $\mR^n$ bestehend aus den Vektoren $\B{v}$ mit $\sum_jv_j=0$. Dann ist $\B{Mv}\in V$ für alle $\B{v}\in V$ und
$$\norm{\B{Mv}}_1\leq c\norm{\B{v}}_1$$
für alle $\B{v}\in V$. Überdies haben wir die Ab\-schätzung
$$c=\max_{1\leq j\leq n}|1-2\min_{1\leq j\leq n}M_{ij}|<1.$$
\end{prop}

\begin{proof}
Wir zeigen zuerst, dass $\B{Mv}\in V$ ist. Sei $\B{w}=\B{Mv}$, also $w_i=\sum_{j=1}^nM_{ij}v_j$ und ausgeschrieben
$$
\sum_{i=1}^nw_i=\sum_{i=1}^n\sum_{j=1}^nM_{ij}v_j
=\sum_{j=1}^nv_j\left(\sum_{i=1}^nM_{ij}\right)=\sum_{j=1}^nv_j=0.
$$
Das heisst $\B{w}=\B{Mv}\in V$.

Um die obere Schranke zu zeigen, halten wir fest, dass
$$\norm{\B{w}}_1=\sum_{i=1}^ne_iw_i=\sum_{i=1}^ne_i\left(\sum_{j=1}^nM_{ij}v_j\right),$$
wobei $e_i=\text{sgn}(w_i)$. Ferner ist
$$\norm{\B{w}}_1=\sum_{j=1}^nv_j\left(\sum_{i=1}^ne_iM_{ij}\right)=\sum_{j=1}^na_jv_j,$$
mit $a_j=\sum_{i=1}^ne_iM_{ij}$. 
Die $e_i$ haben nicht alle das gleiche Vorzeichen, da ja $\sum_iw_i=0$ ist (für $\B{w}=\B{0}$ ist der Fall eh klar). Wegen $\sum_iM_{ij}=1$ mit $0<M_{ij}<1$ folgt
$$
-1<-1+2\min_{1\leq i\leq n}M_{ij}\leq a_j
\leq1-2\min_{1\leq i\leq n}M_{ij}<1
$$
und daher
$$\abs{a_j}\leq |1-2\min_{1\leq i\leq n}M_{ij}|<1.$$
Setze $c=\max_{1\leq j\leq n}\abs{1-2\min_{1\leq j\leq n}M_{ij}}$. Es ist $c<1$ und $\abs{a_j}\leq c$ für alle $j$, was schliesslich
$$
\norm{\B{w}}_1=\sum_{j=1}^na_jv_j=\abs{\sum_{j=1}^na_jv_j}\leq\\
\leq\sum_{j=1}^n\abs{a_j}\abs{v_j}\leq c\sum_{j=1}^n\abs{v_j}=c\norm{\B{v}}_1
$$
liefert.
\end{proof}

Proposition \ref{satzbound} leitet über zu

\begin{satz}
Zu jeder positiven spalten-stochastischen Matrix $\B{M}$ gibt es einen eindeutig bestimmten Vektor $\B{q}$ mit lauter positiven Komponenten, so dass $\B{Mq}=\B{q}$ und $\norm{\B{q}}_1=1$. Und, $\B{q}$ ist der Grenzwert $\lim_{k\to\infty}\B{M}^k\B{x}_0$ für einen beliebigen Initalvektor $\B{x}_0$ mit lauter positiven Komponenten und $\norm{\B{x}_0}_1=1$.
\end{satz}

Ein Highlight, dessen Beweis wir kaum erwarten können.

\begin{proof}
Nach Proposition \ref{satzspaltenstochastischev1} hat $\B{M}$ einen Eigenwert $1$ und nach Proposition \ref{thmM} ist $V_1(\B{M})$ eindimensional. Ferner haben alle nichttrivialen Vektoren in $V_1(\B{M})$ entweder lauter positive oder lauter negative Komponenten. Also existiert ein eindeutig bestimmter Vektor $\B{q}\in V_1(\B{M})$ mit lauter positiven Komponenten und $\norm{\B{q}}_1=1$. Sei $\B{x}_0\in\mR^n$ mit positiven Komponenten und $\norm{\B{x}_0}_1=1$ beliebig. Wir notieren $\B{x}_0=\B{q}+\B{v}$ mit $\B{v}\in V$ wie in Satz \ref{satzbound}. Weiter ist $\B{M}^k\B{x}_0=\B{M}^k\B{q}+\B{M}^k\B{v}=\B{q}+\B{M}^k\B{v}$. Daraus folgt
$$\B{M}^k\B{x}_0-\B{q}=\B{M}^k\B{v}.$$
Eine vollständige Induktion und Proposition \ref{thmM} liefern $\norm{\B{M}^k\B{v}}_1\leq c^k\norm{\B{v}}_1$ für $0\leq c<1$ und damit $\lim_{k\to\infty}\norm{\B{M}^k\B{v}}_1=0.$ Mit obiger Gleichung haben wir $\lim_{k\to\infty}\B{M}^k\B{x}_0=\B{q}$.
\end{proof}

\begin{bsp}
Sei $\B{M}$ die erweiterte Link-Matrix zum Web in Abbildung \ref{bspnonunique}. Wir wählen $\B{x}_0=[0.24\;0.31\;0.08\;0.18\;0.19]^{T}$ als Initialvektor; wir hatten $\B{q}=[0.2\;0.2\;0.285\;0.285\;0.03]^{T}$. In der Tabelle \ref{tabkonvergenz} finden Sie die Werte von $\norm{\B{M}^k\B{x}_0-\B{q}}_1$ und $\frac{\norm{\B{M}^k\B{x}_0-\B{q}}_1}{\norm{\B{M}^{k-1}\B{x}_0-\B{q}}_1}$. Vergleichen Sie diese Werte mit der Schranke $c$ aus Proposition \ref{satzbound}, welche in diesem Fall $0.94$ ist.

\begin{table}
\begin{center}
\begin{tabular}{c|c|c}
$k$	&	$\norm{\B{M}^k\B{x}_0-\B{q}}_1$	&	$\frac{\norm{\B{M}^k\B{x}_0-\B{q}}_1}{\norm{\B{M}^{k-1}\B{x}_0-\B{q}}_1}$	\\	\hline
$0$	&	$0.62$	&		\\
$1$	&	$0.255$	&	$0.411$	\\
$5$	&	$0.133$	&	$0.85$	\\
$10$	&	$0.0591$	&	$0.85$\\
$50$	&	$8.87\cdot10^{-5}$	&	$0.85$
\end{tabular}
\caption{Betrachtung der Konvergenz}\label{tabkonvergenz}
\end{center}
\end{table}
\end{bsp}

Die Schranke $\norm{\B{M}^k\B{x}_0-\B{q}}_1\leq c^k\norm{\B{x}_0-\B{q}}_1$ ist vorsichtig gewählt ($0.85$ ist der Wert von $1-m$ und $0.85$ stellt sich als zweitgrösster Eigenwert von $\B{M}$ heraus). Man kann zeigen, dass im Allgemeinen die power method asymptotisch mit $\norm{\B{Mx}_k-\B{q}}_1\approx\abs{\gl_2}\norm{\B{x}-\B{q}}_1$ wobei $\gl_2$ der zweitgrösste Eigenwert von $\B{M}$ ist, konvergiert. Überdies kann man für ein $\B{M}$ der Form $\B{M}=(1-m)\B{A}+m\B{S}$ mit $\B{A}$ spalten-stochastisch und $S_{ij}=\frac{1}{n}$ für alle $i,j$ zeigen, dass $\abs{\gl_2}\leq1-m$ ist. Das heisst die power method wird schneller konvergieren als $c^k\norm{\B{x}_0-\B{q}}_1$ suggeriert. Trotzdem liefert das $c$ eine einfache Schranke für unsere Zwecke. Weil jeder Eintrag in $\B{M}$ mindestens den Wert $\frac{m}{n}$ hat ist klar $c\leq1-2\frac{m}{n}$.

In der Praxis wird die positive Matrix $\B{M}$ keine trivialen Komponenten enthalten, weshalb eine Multiplikation $\B{M}\B{v}$ mit $\B{v}\in\mR^n$ typischerweise $O(n^2)$ Multiplikationen und Additionen erfordert; im Falle von $n=8\cdot10^9$ ein beträchtlicher Aufwand. Umgehen kann man dies mit Gleichung \eqref{equs}, wenn man einen positiven Vektor $\B{x}$ mit $\norm{\B{x}}_1=1$ wählt, da dann $\B{Mx}$ äquivalent zu $(1-m)\B{Ax}+m\B{s}$ ist. Diese Rechnung ist weitaus effizienter, weil in $\B{A}$ die meisten Komponenten gleich $0$ sein werden; die meisten Websites linken ja zu bloss einigen wenigen Pages.

Wir haben nun unser Ziel bewiesen:

\begin{thm}
Die Matrix $\B{M}$ definiert wie in \eqref{equM} für ein Web ohne lose Pages ist eine positive spalten-stochastische Matrix und hat daher einen eindeutig bestimmten Ranking-Vektor $\B{q}$ mit $\sum_iq_i=1$, so dass $\B{Mq}=\B{q}$ gilt. Dieser Ranking-Vektor $\B{q}$ kann als Grenzwert der Iteration $\B{x}_k=(1-m)\B{Ax}_{k-1}+m\B{s}$ berechnet werden, wobei $\B{x}_0$ ein Initialvektor mit positiven Komponenten und $\norm{\B{x}_0}_1=1$ ist.
\end{thm}

Der Vektor $\B{x}$ definiert wie in \eqref{equs} kann auch stochastisch interpretiert werden. Ein Web-Surfer auf einem Web mit $n$ Pages, ohne lose Pages, befinde sich auf einer dieser Pages, und er bewege sich zufällig in diesem Netz nach folgendem Muster: Befindet sich der Surfer auf einer Page mit $r$ Links, wählt er zufällig einen dieser $r$ Links mit Wahrscheinlichkeit $\frac{1-m}{r}$, oder aber er wechselt zu einer zufälligen Page im Web, wobei jede Gewichtung $\frac{m}{n}$ hat. Wegen $r\cdot\frac{1-m}{r}+n\cdot\frac{m}{n}=1=100\%$ hat er in unserem Model nur diese Möglichkeiten. Dieses Muster wiederhole sich nun beliebig oft. Somit entsprechen die Komponenten $x_j$ im normierten Vektor $\B{x}$ aus \eqref{equs} entspricht langfristig dem mittleren Zeitanteil, der der Surfer auf Page $j$ verbringt. Relevantere Pages werden meistens öfter verlinkt als weniger relevante, weshalb der Surfer die relevanteren häufiger besucht.

\begin{ueb}
Berechne für das Web aus Übung \ref{uebnewrank} für $k=1,5,10,50$ die Werte $\norm{\B{M}^k\B{x}_0-\B{q}}_1$ und $\frac{\norm{\B{M}^k\B{x}_0-\B{q}}_1}{\norm{\B{M}^{k-1}\B{x}_0-\B{q}}_1}$. Wähle einen Initialvektor, der nicht zu nahe am Eigenvektor ist, damit du die Konvergenz verfolgen kannst. Bestimme $c$ und den Absolutwert des zweitgrössten Eigenwerts von $\B{M}$.
\end{ueb}

\begin{ueb}
Betrachte die Link-Matrix
$$\B{A}=
\begin{pmatrix}
0	&	\frac{1}{2}	&	\frac{1}{2}	\\[0.5ex]
0	&	0		&	\frac{1}{2}	\\[0.5ex]
1	&	\frac{1}{2}	&	0
\end{pmatrix}
.$$
Zeige, dass $\B{M}=(1-m)\B{A}+m\B{S}$ ($S_{ij}=\frac{1}{3}$) für $0\leq m<1$ nicht diagonalisierbar ist.
\end{ueb}

\begin{ueb}
Wie sollte $m$ gewählt werden? Welchen Einfluss hat $m$ auf das Ranking bzw. die Berechnungszeit?
\end{ueb}